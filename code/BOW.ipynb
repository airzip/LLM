{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本特征处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ')' does not match opening parenthesis '[' (956036434.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[39], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    txt_list.append(str([j.strip() for j in i.split(','))])\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis ')' does not match opening parenthesis '['\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with  open('/home/its/CodeReview/LLM/data/kapok.txt', 'r', encoding='utf-8') as txt_file:\n",
    "    #txt_list = []\n",
    "    for i in txt_file:\n",
    "        txt_list.append([j.strip() for j in i.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['致橡树'],\n",
       " ['【作者】舒婷 【朝代】现代'],\n",
       " ['我如果爱你——'],\n",
       " ['绝不像攀援的凌霄花，'],\n",
       " ['借你的高枝炫耀自己；'],\n",
       " ['我如果爱你——'],\n",
       " ['绝不学痴情的鸟儿，'],\n",
       " ['为绿荫重复单调的歌曲；'],\n",
       " ['也不止像泉源，'],\n",
       " ['常年送来清凉的慰藉；'],\n",
       " ['也不止像险峰，'],\n",
       " ['增加你的高度，衬托你的威仪。'],\n",
       " ['甚至日光，'],\n",
       " ['甚至春雨。'],\n",
       " [''],\n",
       " ['不，这些都还不够！'],\n",
       " ['我必须是你近旁的一株木棉，'],\n",
       " ['作为树的形象和你站在一起。'],\n",
       " ['根，紧握在地下；'],\n",
       " ['叶，相触在云里。'],\n",
       " ['每一阵风过，'],\n",
       " ['我们都互相致意，'],\n",
       " ['但没有人，'],\n",
       " ['听懂我们的言语。'],\n",
       " ['你有你的铜枝铁干，'],\n",
       " ['像刀，像剑，也像戟；'],\n",
       " ['我有我红硕的花朵，'],\n",
       " ['像沉重的叹息，'],\n",
       " ['又像英勇的火炬。'],\n",
       " [''],\n",
       " ['我们分担寒潮、风雷、霹雳；'],\n",
       " ['我们共享雾霭、流岚、虹霓。'],\n",
       " ['仿佛永远分离，'],\n",
       " ['却又终身相依。'],\n",
       " ['这才是伟大的爱情，'],\n",
       " ['坚贞就在这里：'],\n",
       " ['爱——'],\n",
       " ['不仅爱你伟岸的身躯，'],\n",
       " ['也爱你坚持的位置，'],\n",
       " ['足下的土地。'],\n",
       " ['致橡树'],\n",
       " ['【作者】舒婷 【朝代】现代'],\n",
       " ['我如果爱你——'],\n",
       " ['绝不像攀援的凌霄花，'],\n",
       " ['借你的高枝炫耀自己；'],\n",
       " ['我如果爱你——'],\n",
       " ['绝不学痴情的鸟儿，'],\n",
       " ['为绿荫重复单调的歌曲；'],\n",
       " ['也不止像泉源，'],\n",
       " ['常年送来清凉的慰藉；'],\n",
       " ['也不止像险峰，'],\n",
       " ['增加你的高度，衬托你的威仪。'],\n",
       " ['甚至日光，'],\n",
       " ['甚至春雨。'],\n",
       " [''],\n",
       " ['不，这些都还不够！'],\n",
       " ['我必须是你近旁的一株木棉，'],\n",
       " ['作为树的形象和你站在一起。'],\n",
       " ['根，紧握在地下；'],\n",
       " ['叶，相触在云里。'],\n",
       " ['每一阵风过，'],\n",
       " ['我们都互相致意，'],\n",
       " ['但没有人，'],\n",
       " ['听懂我们的言语。'],\n",
       " ['你有你的铜枝铁干，'],\n",
       " ['像刀，像剑，也像戟；'],\n",
       " ['我有我红硕的花朵，'],\n",
       " ['像沉重的叹息，'],\n",
       " ['又像英勇的火炬。'],\n",
       " [''],\n",
       " ['我们分担寒潮、风雷、霹雳；'],\n",
       " ['我们共享雾霭、流岚、虹霓。'],\n",
       " ['仿佛永远分离，'],\n",
       " ['却又终身相依。'],\n",
       " ['这才是伟大的爱情，'],\n",
       " ['坚贞就在这里：'],\n",
       " ['爱——'],\n",
       " ['不仅爱你伟岸的身躯，'],\n",
       " ['也爱你坚持的位置，'],\n",
       " ['足下的土地。']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus = [\n",
    " '致橡树',\n",
    " '【作者】舒婷 【朝代】现代',\n",
    " '我如果爱你——',\n",
    " '绝不像攀援的凌霄花，',\n",
    " '借你的高枝炫耀自己；',\n",
    " '我如果爱你——',\n",
    " '绝不学痴情的鸟儿，',\n",
    " '为绿荫重复单调的歌曲；',\n",
    " '也不止像泉源，',\n",
    " '常年送来清凉的慰藉；',\n",
    " '也不止像险峰，',\n",
    " '增加你的高度，衬托你的威仪。',\n",
    " '甚至日光，',\n",
    " '甚至春雨。'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "\n",
    "jieba.enable_parallel(8)\n",
    "#jieba.load_userdict('')\n",
    "jieba.analyse.set_stop_words('/home/its/CodeReview/LLM/data/baidu_stopwords.txt')\n",
    "for sentence in raw_corpus:\n",
    "    sentence = ''.join(re.findall(r'[\\u4e00-\\u9fa5]+', sentence))\n",
    "    corpus.append([item for item in jieba.cut(sentence)])\n",
    "\n",
    "#[print([jieba.cut(word)]) for word in raw_corpus ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<generator object _pcut at 0x7fee9b909d40>,\n",
       " ['致橡树'],\n",
       " ['作者', '舒婷', '朝代', '现代'],\n",
       " ['我', '如果', '爱', '你'],\n",
       " ['绝不', '像', '攀援', '的', '凌霄花'],\n",
       " ['借', '你', '的', '高枝', '炫耀', '自己'],\n",
       " ['我', '如果', '爱', '你'],\n",
       " ['绝不', '学', '痴情', '的', '鸟儿'],\n",
       " ['为', '绿荫', '重复', '单调', '的', '歌曲'],\n",
       " ['也', '不止', '像', '泉源'],\n",
       " ['常年', '送来', '清凉', '的', '慰藉'],\n",
       " ['也', '不止', '像', '险峰'],\n",
       " ['增加', '你', '的', '高度', '衬托', '你', '的', '威仪'],\n",
       " ['甚至', '日光'],\n",
       " ['甚至', '春雨'],\n",
       " ['致橡树'],\n",
       " ['作者', '舒婷', '朝代', '现代'],\n",
       " ['我', '如果', '爱', '你'],\n",
       " ['绝不', '像', '攀援', '的', '凌霄花'],\n",
       " ['借', '你', '的', '高枝', '炫耀', '自己'],\n",
       " ['我', '如果', '爱', '你'],\n",
       " ['绝不', '学', '痴情', '的', '鸟儿'],\n",
       " ['为', '绿荫', '重复', '单调', '的', '歌曲'],\n",
       " ['也', '不止', '像', '泉源'],\n",
       " ['常年', '送来', '清凉', '的', '慰藉'],\n",
       " ['也', '不止', '像', '险峰'],\n",
       " ['增加', '你', '的', '高度', '衬托', '你', '的', '威仪'],\n",
       " ['甚至', '日光'],\n",
       " ['甚至', '春雨'],\n",
       " ['致橡树'],\n",
       " ['作者', '舒婷', '朝代', '现代'],\n",
       " ['我', '如果', '爱', '你'],\n",
       " ['绝不', '像', '攀援', '的', '凌霄花'],\n",
       " ['借', '你', '的', '高枝', '炫耀', '自己'],\n",
       " ['我', '如果', '爱', '你'],\n",
       " ['绝不', '学', '痴情', '的', '鸟儿'],\n",
       " ['为', '绿荫', '重复', '单调', '的', '歌曲'],\n",
       " ['也', '不止', '像', '泉源'],\n",
       " ['常年', '送来', '清凉', '的', '慰藉'],\n",
       " ['也', '不止', '像', '险峰'],\n",
       " ['增加', '你', '的', '高度', '衬托', '你', '的', '威仪'],\n",
       " ['甚至', '日光'],\n",
       " ['甚至', '春雨']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "dictionary.save('/home/its/CodeReview/LLM/data/love.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=20, no_above=0.5)  # 删掉只在不超过20个文本中出现过的词，删掉在50%及以上的文本都出现了的词\n",
    "# dictionary.filter_tokens(['一个'])  # 这个函数可以直接删除指定的词\n",
    "dictionary.compactify()  # 去掉因删除词汇而出现的空白"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(s) for s in corpus]\n",
    "corpora.MmCorpus.serialize('corpus_bow.mm', corpus)  # 存储语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m LdaModel\n\u001b[0;32m----> 3\u001b[0m temp \u001b[39m=\u001b[39m dictionary[\u001b[39m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m id2word \u001b[39m=\u001b[39m dictionary\u001b[39m.\u001b[39mid2token\n\u001b[1;32m      6\u001b[0m model \u001b[39m=\u001b[39m LdaModel(\n\u001b[1;32m      7\u001b[0m     corpus\u001b[39m=\u001b[39mcorpus,\n\u001b[1;32m      8\u001b[0m     id2word\u001b[39m=\u001b[39mid2word,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     eval_every\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m/data/home/its/VirtualReality/conda/miniforge/envs/bion/lib/python3.11/site-packages/gensim/corpora/dictionary.py:107\u001b[0m, in \u001b[0;36mDictionary.__getitem__\u001b[0;34m(self, tokenid)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid2token) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken2id):\n\u001b[1;32m    104\u001b[0m     \u001b[39m# the word->id mapping has changed (presumably via add_documents);\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[39m# recompute id->word accordingly\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid2token \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mrevdict(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken2id)\n\u001b[0;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid2token[tokenid]\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "temp = dictionary[0]\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=2000,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=500,\n",
    "    num_topics=3,\n",
    "    passes=20,\n",
    "    eval_every=None\n",
    ")\n",
    "\n",
    "# model.save('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m top_topics \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtop_topics(corpus) \u001b[39m#, num_words=20)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m avg_topic_coherence \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m([t[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m top_topics]) \u001b[39m/\u001b[39m num_topics\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "vis = pyLDAvis.gensim.prepare(model, corpus, dictionary)\n",
    "# 需要的三个参数都可以从硬盘读取的，前面已经存储下来了\n",
    "\n",
    "pyLDAvis.show(vis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
